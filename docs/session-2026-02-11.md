# Session Summary — 2026-02-11

## Goals

1. **Implement the full AIMD bandwidth-adaptation design** from `docs/bandwidth-adaptation-design.md` into the DWRR scheduler.
2. **Release v0.3.0** with the AIMD feature.
3. **Build an integration test** verifying bandwidth differentiation across three bonded RIST links with distinct capacities (500 / 1200 / 1750 kbps ratio).

## Completed Work

### 1. AIMD Capacity Estimator (v0.3.0)

Implemented the delay-gradient + loss-based multiplicative-decrease congestion control system in `rist-bonding-core`:

- **`scheduler/dwrr.rs`** — Full AIMD state machine per link: additive increase on clean RTT, multiplicative decrease on loss or rising delay-gradient, cooldown guard, configurable α/β/thresholds.
- **`stats.rs`** — Extended stats schema (v3) with `estimated_capacity_bps`, `delay_gradient_ms`, `loss_rate`, `aimd_state` per link.
- **`config.rs`** — `AimdConfig` struct with tuneable parameters.
- 13 AIMD-specific unit tests + 157 total core tests passing.
- Fixed clippy `large_enum_variant` warning (`RuntimeMessage::ApplyConfig(Box<BondingConfig>)`).
- Tagged and pushed as **v0.3.0**.

### 2. Netem Bandwidth Enforcement in `rist-network-sim`

Discovered that `tc netem rate Xkbit` with the default queue limit (1000 packets)
only adds serialization delay — the queue never fills in short tests, so bandwidth
is never actually enforced.  Solved by reducing netem's `limit` parameter:

- Added `netem_limit: Option<u32>` field to `ImpairmentConfig`.
- When `rate_kbit` is set and `netem_limit` is `None`, an appropriate limit is
  **auto-calculated from the bandwidth-delay product** (~2× BDP, minimum 20 packets).
- The finite queue causes netem to **drop excess packets at the qdisc level** —
  these drops are visible to the receiver, producing RTCP NACKs that enable AIMD
  convergence.
- Removed the previous TBF (Token Bucket Filter) approach entirely.  TBF drops
  were invisible to the application (kernel qdisc drops after `sendto()` returns
  success), so AIMD could never converge.

### 3. Three-Link Bandwidth Differentiation Test

New integration test `test_three_link_bandwidth_differentiation` in `impaired_e2e.rs`:

- Creates 3 veth pairs between sender/receiver namespaces at 5 / 12 / 17.5 Mbps.
- Netem enforces bandwidth via `rate` + finite `limit`; drops are visible to RTCP.
- Runs a 20-second bonded RIST video stream.
- Verifies via `tc -s` stats that throughput ordering matches link bandwidth ordering.
- Asserts all 3 links are alive and carrying traffic.

## Files Changed

| File | Change |
|------|--------|
| `crates/rist-network-sim/src/impairment.rs` | `netem_limit` field, BDP-based auto-limit, removed TBF |
| `crates/gst-rist-bonding/tests/impaired_e2e.rs` | New `test_three_link_bandwidth_differentiation` |
| `docs/session-2026-02-11.md` | This summary |

## Test Results

- **157 unit tests**: all passing
- **4 rist-network-sim tests**: all passing
- **3 impaired_e2e integration tests**: all passing (step_change, 3-link, impaired_bonding)
- Pre-commit hooks (rustfmt + clippy): clean
